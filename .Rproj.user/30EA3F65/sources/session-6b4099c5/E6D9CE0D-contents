---
title: "Correlations"
author: "Hannah Corenblum"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE)

library(nlme)
library(multilevel)
library(psychometric)
library(boot)
library(gt)
library(readxl)
library(ggplot2)


#Cognition norms set #1: cognition norms that have the regular number of participant ratings (i.e. 10-38 ratings)
#Cognition norms set #2: cognition norms that have the large number of participant ratings (i.e. around 120 ratings per word)


#import ratings: these contains words that gone through the data cleaning process. These are all the raw ratings from set 2
ratings_correlationstability <- read_excel("./ratings_icc_second.xlsx")


#all words from set 2 not including control words 
descriptives2_second <- read_excel("./noncontrolssettwo.xlsx")


#These are the control words from set 2

controls_second <- read_excel("./controls_second.xlsx")

#These are Binder et al., 2016 cognition norms 
binderdata <- read_excel("./bindercontrol.xlsx")

#clean up object
binderdata <- dplyr::select(binderdata, words, cognition )
binderdata <- binderdata %>%
  mutate(words = ifelse(words == "broccoli", "brocolli", words))
binderdata <- binderdata %>%
  rename(spreadsheet_word = words)


```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


```{r icc, cache=TRUE, include=FALSE}



attach(ratings_correlationstability)
mod <- lme(response ~ 1, random = ~1 | spreadsheet_word, na.action = na.omit, control = lmeControl(opt = "optim"))
detach(ratings_correlationstability)


#Extract intercept variance
t0 <- as.numeric(VarCorr(mod)[1,1])


#Extract residual variance
sig2 <- as.numeric(VarCorr(mod)[2,1])


#Calculate ICC based on intercept and residual variance
icc1 <- t0/(t0 + sig2)



#Calculate mean ICC across all group ICCs 
#679 is sample size:

icc2 <- (679*icc1)/(1+((679-1)*icc1))



```


The intercept variance is `t0`

The residual variance is `sig2`

The ICC based on intercept and residual variance is `icc1`

The mean ICC across all group. ICCs is `icc2`

##  AT WHAT SAMPLE SIZE DOES THE MEAN STABILIZE

```{r mean_of_means}


library(dplyr)
library(purrr)

# Function to calculate mean and SD for participant ratings at different sample sizes
calculate_mean_sd_aggregated <- function(data, min_sample_size, max_sample_size = 125, n_bootstrap=100) {
  results <- data.frame(Mean = numeric(), SD = numeric(), SampleSize = integer())
  
  # Corrected loop to use the correct variable and syntax
  for (sample_size in min_sample_size:max_sample_size) {
    # Bootstrap sampling at the level of participant ratings for each word
    bootstrap_means <- replicate(n_bootstrap, {
      # Randomly sample 'sample_size' participant ratings for each word, compute the mean for each word, then average those means
      sampled_means <- data %>%
        group_by(spreadsheet_word) %>%
        summarise(WordMean = mean(sample(response, size = sample_size, replace = TRUE), na.rm = TRUE), .groups = 'drop') %>%
        pull(WordMean)
        
      mean(sampled_means, na.rm = TRUE)
    })
    
    # Calculate means and standard deviations of bootstrap means
    mean_of_means <- mean(bootstrap_means)
    sd_of_means <- sd(bootstrap_means)
    
    # Append results
    results <- rbind(results, data.frame(SampleSize = sample_size, Mean = mean_of_means, SD = sd_of_means))
  }
  
  return(results)
}

# Assuming you have a dataset called 'ratings_correlationstability' with columns 'spreadsheet_word' and 'response'
# Process aggregated results
results_df <- calculate_mean_sd_aggregated(ratings_correlationstability, min_sample_size = 3)

# t re sults
print(results_df)

#put results into a sliced dataframe

# Filter specific rows 
results_df_table <- results_df %>%
  slice(c(1, 3, 8, 13, 23, 48, 73, 98, 118))


# Apply gt() for table formatting
results_df_table <- gt(results_df_table) %>%
  cols_label(
    Mean = "M",
    SampleSize = "n"
  ) %>%
  fmt_number(
    columns = vars(SampleSize),
    decimals = 0  # No decimals for the "n" column
  ) %>%
  fmt_number(
    columns = vars(Mean, SD),
    decimals = 3  # Set decimal places for Mean and SD
  ) %>%
  tab_options(
    table.width = pct(35))

# Print the results in a table format
print(results_df_table)



# Plotting with mean as a continuous line and SD as error bars
plot <- ggplot(data = results_df, aes(x = SampleSize)) +
  geom_line(aes(y = Mean, color = "Mean"), size = 1) +  # Mean line with legend
  geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD, color = "Standard Deviation"), width = 1) +  # SD bars with legend
  scale_color_manual(values = c(name = "Legend", "Mean" = "blue", "Standard Deviation" = "red"), labels = c("Mean", "Standard Deviation")) +
  labs(title = "Mean and Standard Deviation by Sample Size Across All Words",
       x = "Sample Size",
       y = "Mean Rating Value") +
  theme_minimal() +
  theme(legend.position = "bottom")

# Print the plot
print(plot)


```



```{r plot 2, cache=TRUE, include=FALSE}

#CORRELATION STABILITY ANALYSIS

# Calculate average ratings for each control word
control_avg_ratings <- controls_second %>%
  inner_join(binderdata, by = "spreadsheet_word") %>%
  group_by(spreadsheet_word) %>%
  summarise(average_rating = mean(response, na.rm = TRUE))

# Merge with historical data from binderdata
combined_control_ratings <- left_join(control_avg_ratings, binderdata, by = "spreadsheet_word")

# Function to calculate correlation stability across sample sizes
correlation_stability <- function(data, min_size, max_size, n_bootstrap=100) {
  results <- data.frame(SampleSize = integer(), Correlation = numeric())
  
  for (size in min_size:max_size) {
    boot_correlations <- replicate(n_bootstrap, {
      sample_data <- sample_n(data, size = size, replace = TRUE)
      cor(sample_data$average_rating, sample_data$cognition, use = "complete.obs")
    }, simplify = "array")
    
    # Store the average correlation for this sample size
    results <- rbind(results, data.frame(SampleSize = size, Correlation = mean(boot_correlations, na.rm = TRUE)))
  }
  
  return(results)
}

# Define the range of sample sizes
min_sample_size <- 2
max_sample_size <- 120

# Perform the correlation stability analysis
results_correlation <- correlation_stability(combined_control_ratings, min_sample_size, max_sample_size)


results_correlation_table <- results_correlation %>%
  slice(c(1, 3, 8, 13, 23, 48, 73, 98, 118)) %>%
  dplyr::select(Correlation, SampleSize)

# Apply gt() for table formatting
results_correlation_table <- gt(results_correlation_table) %>%
  cols_label(
    Correlation = "r",
    SampleSize = "n"
  ) %>%
  fmt_number(
    columns = vars(Correlation),
    decimals = 3  # Set decimal places for Mean and SD
  ) %>%
  fmt_number(
    columns = vars(SampleSize),
    decimals = 0  # No decimals for the "n" column
  ) %>%
  tab_options(
    table.width = pct(25))

# Print the results in a table format
print(results_correlation_table)

# Plotting the correlation stability
ggplot(results_correlation, aes(x = SampleSize, y = Correlation)) +
  geom_line() +
  labs(title = "Correlation Stability Analysis", x = "Sample Size", y = "Correlation Coefficient") +
  theme_minimal()

# Print the results table
print(results_correlation)
```

```{r plottwoforshow, echo=FALSE}
ggplot(results_correlation, aes(x = SampleSize, y = Correlation)) +
  geom_line() +
  labs(title = "Correlation Stability Analysis", x = "Sample Size", y = "Correlation Coefficient") +
  theme_minimal()

```

```{r plottwochart, echo=FALSE}
results_correlation
```


